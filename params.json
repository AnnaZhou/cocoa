{"name":"CoCoA","tagline":"communication-efficient distributed coordinate ascent","body":"<!-- # CoCoA & CoCoA+ - Communication-Efficient Distributed Coordinate Ascent-->\r\n\r\nNEW! We've added support for faster additive udpates with CoCoA+. See more information [here](http://arxiv.org/abs/1502.03508).\r\n\r\nThis code performs a comparison of 5 distributed algorithms for training of machine learning models, using [Apache Spark](http://spark.apache.org). The implemented algorithms are\r\n - _CoCoA+_\r\n - _CoCoA_\r\n - mini-batch stochastic dual coordinate ascent (_mini-batch SDCA_)\r\n - stochastic subgradient descent with local updates (_local SGD_)\r\n - mini-batch stochastic subgradient descent (_mini-batch SGD_)\r\n\r\nThe present code trains a standard SVM (hinge-loss, l2-regularized), and reports training and test error, as well as the duality gap certificate if the method is primal-dual.\r\n\r\n## Getting Started\r\nHow to run the code locally:\r\n\r\n```\r\nsbt/sbt assembly\r\n./run-demo-local.sh\r\n```\r\n\r\n(For the `sbt` script to run, make sure you have downloaded CoCoA into a directory whose path contains no spaces.)\r\n\r\n## References\r\nThe CoCoA algorithmic framework is described in more details in the following paper:\r\n\r\n_Jaggi, M., Smith, V., Takac, M., Terhorst, J., Krishnan, S., Hofmann, T., & Jordan, M. I. (2014) [Communication-Efficient Distributed Dual Coordinate Ascent](http://papers.nips.cc/paper/5599-communication-efficient-distributed-dual-coordinate-ascent) (pp. 3068â€“3076). NIPS 2014 - Advances in Neural Information Processing Systems 27._\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}